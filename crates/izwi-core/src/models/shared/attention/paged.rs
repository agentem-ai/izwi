//! Shared paged-KV helpers for decode-time attention.

use candle_core::{DType, Tensor, D};

use crate::error::{Error, Result};

/// Default KV page size used when model-specific config is unavailable.
pub const DEFAULT_KV_PAGE_SIZE: usize = 64;

/// Supported KV cache quantization modes.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum KvCacheQuantization {
    /// Keep KV pages in the model's native dtype.
    None,
    /// Store KV pages in int8 with per-page symmetric scale.
    Int8,
}

impl KvCacheQuantization {
    /// Parse quantization mode from a KV cache dtype hint.
    ///
    /// Accepts values such as `int8`, `i8`, `q8`, `float16`, `float32`, `bf16`.
    pub fn from_dtype_hint(dtype: &str) -> Self {
        match dtype.trim().to_ascii_lowercase().as_str() {
            "int8" | "i8" | "q8" | "q8_0" => Self::Int8,
            _ => Self::None,
        }
    }
}

/// Resolve default page size, optionally overridden by env.
pub fn default_kv_page_size() -> usize {
    std::env::var("IZWI_KV_PAGE_SIZE")
        .ok()
        .and_then(|raw| raw.parse::<usize>().ok())
        .filter(|v| *v > 0)
        .unwrap_or(DEFAULT_KV_PAGE_SIZE)
}

/// Resolve default KV quantization from env.
pub fn default_kv_quantization() -> KvCacheQuantization {
    std::env::var("IZWI_KV_CACHE_DTYPE")
        .ok()
        .map(|raw| KvCacheQuantization::from_dtype_hint(&raw))
        .unwrap_or(KvCacheQuantization::None)
}

/// Single KV page storage (dense or quantized).
#[derive(Debug, Clone)]
pub enum KvPage {
    Dense(Tensor),
    Int8 {
        values: Tensor,
        scale: f32,
        target_dtype: DType,
    },
}

impl KvPage {
    fn from_dense(tensor: Tensor, quantization: KvCacheQuantization) -> Result<Self> {
        match quantization {
            KvCacheQuantization::None => Ok(Self::Dense(tensor)),
            KvCacheQuantization::Int8 => {
                let (values, scale, target_dtype) = quantize_tensor_int8(&tensor)?;
                Ok(Self::Int8 {
                    values,
                    scale,
                    target_dtype,
                })
            }
        }
    }

    fn seq_len(&self) -> Result<usize> {
        match self {
            Self::Dense(t) => t.dim(1).map_err(Error::from),
            Self::Int8 { values, .. } => values.dim(1).map_err(Error::from),
        }
    }

    fn to_dense(&self) -> Result<Tensor> {
        match self {
            Self::Dense(t) => Ok(t.clone()),
            Self::Int8 {
                values,
                scale,
                target_dtype,
            } => dequantize_tensor_int8(values, *scale, *target_dtype),
        }
    }
}

fn quantize_tensor_int8(tensor: &Tensor) -> Result<(Tensor, f32, DType)> {
    let target_dtype = tensor.dtype();
    let max_abs = tensor
        .abs()?
        .max_all()?
        .to_dtype(DType::F32)?
        .to_scalar::<f32>()?;
    let scale = if max_abs > 0.0 {
        (max_abs / 127.0).max(1e-8)
    } else {
        1.0
    };
    let inv_scale = 1.0f32 / scale;
    let inv_scale_t =
        Tensor::from_vec(vec![inv_scale], (1,), tensor.device())?.to_dtype(target_dtype)?;
    let offset_t =
        Tensor::from_vec(vec![128.0f32], (1,), tensor.device())?.to_dtype(target_dtype)?;
    let quantized = tensor
        .broadcast_mul(&inv_scale_t)?
        .clamp(-127.0f64, 127.0f64)?
        .round()?
        .broadcast_add(&offset_t)?
        .to_dtype(DType::U8)?;
    Ok((quantized, scale, target_dtype))
}

fn dequantize_tensor_int8(values: &Tensor, scale: f32, target_dtype: DType) -> Result<Tensor> {
    let dense = values.to_dtype(target_dtype)?;
    let offset_t =
        Tensor::from_vec(vec![128.0f32], (1,), values.device())?.to_dtype(target_dtype)?;
    let dense = dense.broadcast_sub(&offset_t)?;
    if (scale - 1.0).abs() < f32::EPSILON {
        return Ok(dense);
    }
    let scale_t = Tensor::from_vec(vec![scale], (1,), values.device())?.to_dtype(target_dtype)?;
    dense.broadcast_mul(&scale_t).map_err(Error::from)
}

/// Append a `[batch, seq, heads, dim]` tensor into fixed-size pages along `seq`.
///
/// The last existing page is filled first (if not full), then new pages are pushed.
pub fn append_to_pages(
    page_size: usize,
    pages: &mut Vec<KvPage>,
    append: &Tensor,
    quantization: KvCacheQuantization,
) -> Result<()> {
    if page_size == 0 {
        return Err(Error::InvalidInput(
            "KV page size must be greater than zero".to_string(),
        ));
    }
    let seq_len = append.dim(1)?;
    if seq_len == 0 {
        return Ok(());
    }

    let mut cursor = 0usize;
    while cursor < seq_len {
        let mut consumed = false;
        if let Some(last) = pages.last_mut() {
            let last_len = last.seq_len()?;
            if last_len < page_size {
                let take = (page_size - last_len).min(seq_len - cursor);
                let chunk = append.narrow(1, cursor, take)?;
                let last_dense = last.to_dense()?;
                let merged = Tensor::cat(&[&last_dense, &chunk], 1)?;
                *last = KvPage::from_dense(merged, quantization)?;
                cursor += take;
                consumed = true;
            }
        }

        if consumed {
            continue;
        }

        let take = page_size.min(seq_len - cursor);
        let chunk = append.narrow(1, cursor, take)?;
        pages.push(KvPage::from_dense(chunk, quantization)?);
        cursor += take;
    }
    Ok(())
}

/// Materialize paged tensors into a single contiguous `[batch, total_seq, heads, dim]` tensor.
pub fn materialize_pages(pages: &[KvPage]) -> Result<Tensor> {
    if pages.is_empty() {
        return Err(Error::InferenceError(
            "Attempted to materialize empty KV pages".to_string(),
        ));
    }
    if pages.len() == 1 {
        return pages[0].to_dense();
    }
    let mut dense_pages = Vec::with_capacity(pages.len());
    for page in pages {
        dense_pages.push(page.to_dense()?);
    }
    let refs: Vec<&Tensor> = dense_pages.iter().collect();
    Tensor::cat(&refs, 1).map_err(Error::from)
}

/// Compute exact single-token attention over paged K/V without materializing full K/V.
///
/// `q` is `[batch, 1, heads, head_dim]` and page tensors are `[batch, page_seq, heads, head_dim]`.
/// Returns `[batch, 1, heads, head_dim]`.
pub fn paged_decode_attention(
    q: &Tensor,
    k_pages: &[KvPage],
    v_pages: &[KvPage],
    num_heads: usize,
    head_dim: usize,
) -> Result<Tensor> {
    if k_pages.is_empty() || v_pages.is_empty() || k_pages.len() != v_pages.len() {
        return Err(Error::InferenceError(
            "Paged decode attention received invalid KV pages".to_string(),
        ));
    }
    let bsz = q.dim(0)?;
    let q_len = q.dim(1)?;
    if q_len != 1 {
        return Err(Error::InvalidInput(format!(
            "Paged decode attention expects q_len=1, got {}",
            q_len
        )));
    }

    let q = q
        .transpose(1, 2)?
        .reshape((bsz * num_heads, q_len, head_dim))?;
    let scale = (head_dim as f64).sqrt();
    let scale_t = Tensor::from_vec(vec![scale as f32], (1,), q.device())?.to_dtype(q.dtype())?;

    let mut running_max: Option<Tensor> = None; // [bh, 1, 1]
    let mut running_sum: Option<Tensor> = None; // [bh, 1, 1]
    let mut running_out: Option<Tensor> = None; // [bh, 1, d]

    for (k_page, v_page) in k_pages.iter().zip(v_pages.iter()) {
        let k_page = k_page.to_dense()?;
        let v_page = v_page.to_dense()?;
        let page_len = k_page.dim(1)?;
        if page_len == 0 {
            continue;
        }

        let k = k_page
            .transpose(1, 2)?
            .reshape((bsz * num_heads, page_len, head_dim))?;
        let v = v_page
            .transpose(1, 2)?
            .reshape((bsz * num_heads, page_len, head_dim))?;

        let mut scores = q.matmul(&k.transpose(1, 2)?)?;
        scores = scores.broadcast_div(&scale_t)?;

        let page_max = scores.max_keepdim(D::Minus1)?;
        let exp_scores = scores.broadcast_sub(&page_max)?.exp()?;
        let page_sum = exp_scores.sum_keepdim(D::Minus1)?;
        let page_out = exp_scores.matmul(&v)?;

        match (&running_max, &running_sum, &running_out) {
            (None, None, None) => {
                running_max = Some(page_max);
                running_sum = Some(page_sum);
                running_out = Some(page_out);
            }
            (Some(cur_max), Some(cur_sum), Some(cur_out)) => {
                let new_max = cur_max.broadcast_maximum(&page_max)?;
                let cur_scale = cur_max.broadcast_sub(&new_max)?.exp()?;
                let page_scale = page_max.broadcast_sub(&new_max)?.exp()?;

                let new_sum = cur_sum
                    .broadcast_mul(&cur_scale)?
                    .broadcast_add(&page_sum.broadcast_mul(&page_scale)?)?;
                let new_out = cur_out
                    .broadcast_mul(&cur_scale)?
                    .broadcast_add(&page_out.broadcast_mul(&page_scale)?)?;

                running_max = Some(new_max);
                running_sum = Some(new_sum);
                running_out = Some(new_out);
            }
            _ => {
                return Err(Error::InferenceError(
                    "Paged decode attention entered inconsistent running state".to_string(),
                ));
            }
        }
    }

    let running_sum = running_sum.ok_or_else(|| {
        Error::InferenceError("Paged decode attention produced no valid page sum".to_string())
    })?;
    let running_out = running_out.ok_or_else(|| {
        Error::InferenceError("Paged decode attention produced no valid page output".to_string())
    })?;

    let out = running_out.broadcast_div(&running_sum)?;
    out.reshape((bsz, num_heads, q_len, head_dim))?
        .transpose(1, 2)
        .map_err(Error::from)
}

#[cfg(test)]
mod tests {
    use super::*;
    use candle_core::{DType, Device};
    use candle_nn::ops;

    fn max_abs_diff(a: &Tensor, b: &Tensor) -> f32 {
        let a = a
            .to_dtype(DType::F32)
            .unwrap()
            .flatten_all()
            .unwrap()
            .to_vec1::<f32>()
            .unwrap();
        let b = b
            .to_dtype(DType::F32)
            .unwrap()
            .flatten_all()
            .unwrap()
            .to_vec1::<f32>()
            .unwrap();
        a.iter()
            .zip(b.iter())
            .fold(0.0f32, |m, (x, y)| m.max((x - y).abs()))
    }

    #[test]
    fn test_append_to_pages_respects_page_size() {
        let device = Device::Cpu;
        let mut pages = Vec::new();
        let tensor = Tensor::randn(0.0f32, 1.0f32, (1, 10, 2, 4), &device).unwrap();
        append_to_pages(4, &mut pages, &tensor, KvCacheQuantization::None).unwrap();
        assert_eq!(pages.len(), 3);
        assert_eq!(pages[0].seq_len().unwrap(), 4);
        assert_eq!(pages[1].seq_len().unwrap(), 4);
        assert_eq!(pages[2].seq_len().unwrap(), 2);

        let next = Tensor::randn(0.0f32, 1.0f32, (1, 3, 2, 4), &device).unwrap();
        append_to_pages(4, &mut pages, &next, KvCacheQuantization::None).unwrap();
        assert_eq!(pages.len(), 4);
        assert_eq!(pages[2].seq_len().unwrap(), 4);
        assert_eq!(pages[3].seq_len().unwrap(), 1);
    }

    #[test]
    fn test_quantized_materialize_close_to_dense() {
        let device = Device::Cpu;
        let full = Tensor::randn(0.0f32, 1.0f32, (1, 17, 4, 8), &device).unwrap();
        let mut pages = Vec::new();
        append_to_pages(5, &mut pages, &full, KvCacheQuantization::Int8).unwrap();
        let materialized = materialize_pages(&pages).unwrap();
        let diff = max_abs_diff(&materialized, &full);
        assert!(diff < 0.08, "max abs diff was {}", diff);
    }

    #[test]
    fn test_paged_decode_matches_dense_single_token() {
        let device = Device::Cpu;
        let bsz = 2usize;
        let num_heads = 4usize;
        let head_dim = 8usize;
        let total_len = 11usize;

        let q = Tensor::randn(0.0f32, 1.0f32, (bsz, 1, num_heads, head_dim), &device).unwrap();
        let k_full = Tensor::randn(
            0.0f32,
            1.0f32,
            (bsz, total_len, num_heads, head_dim),
            &device,
        )
        .unwrap();
        let v_full = Tensor::randn(
            0.0f32,
            1.0f32,
            (bsz, total_len, num_heads, head_dim),
            &device,
        )
        .unwrap();

        let mut k_pages = Vec::new();
        let mut v_pages = Vec::new();
        append_to_pages(3, &mut k_pages, &k_full, KvCacheQuantization::None).unwrap();
        append_to_pages(3, &mut v_pages, &v_full, KvCacheQuantization::None).unwrap();

        let paged = paged_decode_attention(&q, &k_pages, &v_pages, num_heads, head_dim).unwrap();

        // Dense reference implementation.
        let q_ref = q
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, 1, head_dim))
            .unwrap();
        let k_ref = k_full
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, total_len, head_dim))
            .unwrap();
        let v_ref = v_full
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, total_len, head_dim))
            .unwrap();
        let scale = (head_dim as f64).sqrt();
        let mut scores = q_ref.matmul(&k_ref.transpose(1, 2).unwrap()).unwrap();
        let scale_t = Tensor::from_vec(vec![scale as f32], (1,), &device)
            .unwrap()
            .to_dtype(DType::F32)
            .unwrap();
        scores = scores.broadcast_div(&scale_t).unwrap();
        let weights = ops::softmax(&scores, D::Minus1).unwrap();
        let dense = weights
            .matmul(&v_ref)
            .unwrap()
            .reshape((bsz, num_heads, 1, head_dim))
            .unwrap()
            .transpose(1, 2)
            .unwrap();

        let diff = max_abs_diff(&paged, &dense);
        assert!(diff < 1e-4, "max abs diff was {}", diff);
    }

    #[test]
    fn test_quantized_paged_decode_matches_dense_single_token() {
        let device = Device::Cpu;
        let bsz = 2usize;
        let num_heads = 4usize;
        let head_dim = 8usize;
        let total_len = 13usize;

        let q = Tensor::randn(0.0f32, 1.0f32, (bsz, 1, num_heads, head_dim), &device).unwrap();
        let k_full = Tensor::randn(
            0.0f32,
            1.0f32,
            (bsz, total_len, num_heads, head_dim),
            &device,
        )
        .unwrap();
        let v_full = Tensor::randn(
            0.0f32,
            1.0f32,
            (bsz, total_len, num_heads, head_dim),
            &device,
        )
        .unwrap();

        let mut k_pages = Vec::new();
        let mut v_pages = Vec::new();
        append_to_pages(4, &mut k_pages, &k_full, KvCacheQuantization::Int8).unwrap();
        append_to_pages(4, &mut v_pages, &v_full, KvCacheQuantization::Int8).unwrap();

        let paged = paged_decode_attention(&q, &k_pages, &v_pages, num_heads, head_dim).unwrap();

        // Dense reference implementation.
        let q_ref = q
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, 1, head_dim))
            .unwrap();
        let k_ref = k_full
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, total_len, head_dim))
            .unwrap();
        let v_ref = v_full
            .transpose(1, 2)
            .unwrap()
            .reshape((bsz * num_heads, total_len, head_dim))
            .unwrap();
        let scale = (head_dim as f64).sqrt();
        let mut scores = q_ref.matmul(&k_ref.transpose(1, 2).unwrap()).unwrap();
        let scale_t = Tensor::from_vec(vec![scale as f32], (1,), &device)
            .unwrap()
            .to_dtype(DType::F32)
            .unwrap();
        scores = scores.broadcast_div(&scale_t).unwrap();
        let weights = ops::softmax(&scores, D::Minus1).unwrap();
        let dense = weights
            .matmul(&v_ref)
            .unwrap()
            .reshape((bsz, num_heads, 1, head_dim))
            .unwrap()
            .transpose(1, 2)
            .unwrap();

        let diff = max_abs_diff(&paged, &dense);
        assert!(diff < 0.12, "max abs diff was {}", diff);
    }
}
